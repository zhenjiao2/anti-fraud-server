"""
åæ¬ºè¯ˆè¯„ä¼°è„šæœ¬ - é¢„æµ‹rating vs äººå·¥æ ‡æ³¨human_ratingçš„å…¨é¢è¯„ä¼°
ä¸“é—¨é’ˆå¯¹åæ¬ºè¯ˆåœºæ™¯è®¾è®¡çš„è¯„ä¼°æŒ‡æ ‡

ä¸»æŒ‡æ ‡ï¼š
- QWK (Quadratic Weighted Kappa)
- MAE (Mean Absolute Error)  
- Weighted F1 / Macro F1

è¾…åŠ©æŒ‡æ ‡ï¼š
- Recall@HighRisk (å¯¹çœŸå®=4/5çš„å¬å›ç‡)
- FPR@LowRisk (çœŸå®=1/2çš„è¯¯åˆ¤ç‡)
- Cost-aware Error (æˆæœ¬åŠ æƒè¯¯å·®)
- Confusion Matrix (ç‰¹åˆ«å…³æ³¨é”™ä½æ–¹å‘)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    cohen_kappa_score, 
    mean_absolute_error, 
    f1_score, 
    confusion_matrix,
    classification_report,
    recall_score,
    precision_score
)
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

def load_and_preprocess_data(file_path: str):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print("=== æ•°æ®åŠ è½½å’Œé¢„å¤„ç† ===")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
    
    # æ£€æŸ¥åˆ—å
    print(f"åˆ—å: {df.columns.tolist()}")
    
    # æ•°æ®æ¸…æ´— - ç§»é™¤ç¼ºå¤±å€¼
    original_len = len(df)
    df = df.dropna(subset=['human rating', 'rating'])
    print(f"ç§»é™¤ç¼ºå¤±å€¼å: {len(df)} æ¡ (ç§»é™¤äº† {original_len - len(df)} æ¡)")
    
    # ç¡®ä¿è¯„åˆ†åœ¨1-5èŒƒå›´å†…
    df = df[(df['human rating'] >= 1) & (df['human rating'] <= 5)]
    df = df[(df['rating'] >= 1) & (df['rating'] <= 5)]
    print(f"ç­›é€‰1-5è¯„åˆ†å: {len(df)} æ¡")
    
    # è½¬æ¢ä¸ºæ•´æ•°ï¼ˆå››èˆäº”å…¥ï¼‰
    df['human_rating_int'] = df['human rating'].round().astype(int)
    df['rating_int'] = df['rating'].round().astype(int)
    
    print(f"\\näººå·¥æ ‡æ³¨åˆ†å¸ƒ:")
    print(df['human_rating_int'].value_counts().sort_index())
    print(f"\\né¢„æµ‹ç»“æœåˆ†å¸ƒ:")
    print(df['rating_int'].value_counts().sort_index())
    
    return df

def calculate_main_metrics(y_true, y_pred):
    """è®¡ç®—ä¸»è¦æŒ‡æ ‡"""
    print("\\n=== ä¸»è¦æŒ‡æ ‡ ===")
    
    # 1. QWK (Quadratic Weighted Kappa)
    qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')
    print(f"QWK (Quadratic Weighted Kappa): {qwk:.4f}")
    
    # QWKè§£é‡Š
    if qwk >= 0.8:
        qwk_interpretation = "ä¼˜ç§€"
    elif qwk >= 0.6:
        qwk_interpretation = "è‰¯å¥½"
    elif qwk >= 0.4:
        qwk_interpretation = "ä¸­ç­‰"
    elif qwk >= 0.2:
        qwk_interpretation = "ä¸€èˆ¬"
    else:
        qwk_interpretation = "è¾ƒå·®"
    print(f"QWKè¯„çº§: {qwk_interpretation}")
    
    # 2. MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"MAE (Mean Absolute Error): {mae:.4f}")
    
    # 3. Weighted F1 å’Œ Macro F1
    weighted_f1 = f1_score(y_true, y_pred, average='weighted')
    macro_f1 = f1_score(y_true, y_pred, average='macro')
    print(f"Weighted F1: {weighted_f1:.4f}")
    print(f"Macro F1: {macro_f1:.4f}")
    
    return {
        'qwk': qwk,
        'qwk_interpretation': qwk_interpretation,
        'mae': mae,
        'weighted_f1': weighted_f1,
        'macro_f1': macro_f1
    }

def calculate_auxiliary_metrics(y_true, y_pred):
    """è®¡ç®—è¾…åŠ©æŒ‡æ ‡"""
    print("\\n=== è¾…åŠ©æŒ‡æ ‡ ===")
    
    # 1. Recall@HighRisk (å¯¹çœŸå®=4/5çš„å¬å›ç‡)
    high_risk_true = (y_true >= 4)
    high_risk_pred = (y_pred >= 4)
    
    if np.sum(high_risk_true) > 0:
        high_risk_recall = np.sum(high_risk_true & high_risk_pred) / np.sum(high_risk_true)
        print(f"Recall@HighRisk (çœŸå®4/5åˆ†çš„å¬å›ç‡): {high_risk_recall:.4f}")
        
        # é«˜é£é™©ç²¾ç¡®ç‡
        if np.sum(high_risk_pred) > 0:
            high_risk_precision = np.sum(high_risk_true & high_risk_pred) / np.sum(high_risk_pred)
            print(f"Precision@HighRisk (é¢„æµ‹4/5åˆ†çš„ç²¾ç¡®ç‡): {high_risk_precision:.4f}")
        else:
            high_risk_precision = 0.0
            print(f"Precision@HighRisk: æ— é¢„æµ‹ä¸ºé«˜é£é™©çš„æ ·æœ¬")
    else:
        high_risk_recall = 0.0
        high_risk_precision = 0.0
        print("æ— çœŸå®é«˜é£é™©æ ·æœ¬")
    
    # 2. FPR@LowRisk (çœŸå®=1/2çš„è¯¯åˆ¤ç‡ä¸ºé«˜é£é™©)
    low_risk_true = (y_true <= 2)
    low_risk_pred = (y_pred <= 2)
    false_positive_high = low_risk_true & (y_pred >= 4)
    
    if np.sum(low_risk_true) > 0:
        fpr_low_risk = np.sum(false_positive_high) / np.sum(low_risk_true)
        print(f"FPR@LowRisk (çœŸå®1/2åˆ†è¢«è¯¯åˆ¤ä¸º4/5åˆ†çš„æ¯”ä¾‹): {fpr_low_risk:.4f}")
        
        # ä½é£é™©å¬å›ç‡å’Œç²¾ç¡®ç‡
        if np.sum(low_risk_pred) > 0:
            low_risk_recall = np.sum(low_risk_true & low_risk_pred) / np.sum(low_risk_true)
            low_risk_precision = np.sum(low_risk_true & low_risk_pred) / np.sum(low_risk_pred)
            print(f"Recall@LowRisk (çœŸå®1/2åˆ†çš„å¬å›ç‡): {low_risk_recall:.4f}")
            print(f"Precision@LowRisk (é¢„æµ‹1/2åˆ†çš„ç²¾ç¡®ç‡): {low_risk_precision:.4f}")
        else:
            low_risk_recall = 0.0
            low_risk_precision = 0.0
            print(f"Recall@LowRisk: æ— é¢„æµ‹ä¸ºä½é£é™©çš„æ ·æœ¬")
    else:
        fpr_low_risk = 0.0
        low_risk_recall = 0.0
        low_risk_precision = 0.0
        print("æ— çœŸå®ä½é£é™©æ ·æœ¬")
    
    # 3. é«˜å±å’Œä½å±çš„Macro F1å’ŒWeighted F1
    # åˆ›å»ºäºŒåˆ†ç±»æ ‡ç­¾ç”¨äºè®¡ç®—F1
    # é«˜å±äºŒåˆ†ç±» (4-5 vs å…¶ä»–)
    y_true_high_binary = (y_true >= 4).astype(int)
    y_pred_high_binary = (y_pred >= 4).astype(int)
    
    # ä½å±äºŒåˆ†ç±» (1-2 vs å…¶ä»–)  
    y_true_low_binary = (y_true <= 2).astype(int)
    y_pred_low_binary = (y_pred <= 2).astype(int)
    
    # è®¡ç®—é«˜å±çš„Macro F1å’ŒWeighted F1
    try:
        high_risk_macro_f1 = f1_score(y_true_high_binary, y_pred_high_binary, average='macro')
        high_risk_weighted_f1 = f1_score(y_true_high_binary, y_pred_high_binary, average='weighted')
        print(f"Macro F1@HighRisk (é«˜å±å®å¹³å‡F1): {high_risk_macro_f1:.4f}")
        print(f"Weighted F1@HighRisk (é«˜å±åŠ æƒF1): {high_risk_weighted_f1:.4f}")
    except Exception as e:
        high_risk_macro_f1 = 0.0
        high_risk_weighted_f1 = 0.0
        print(f"é«˜å±Macro/Weighted F1è®¡ç®—å¤±è´¥: {str(e)}")
    
    # è®¡ç®—ä½å±çš„Macro F1å’ŒWeighted F1
    try:
        low_risk_macro_f1 = f1_score(y_true_low_binary, y_pred_low_binary, average='macro')
        low_risk_weighted_f1 = f1_score(y_true_low_binary, y_pred_low_binary, average='weighted')
        print(f"Macro F1@LowRisk (ä½å±å®å¹³å‡F1): {low_risk_macro_f1:.4f}")
        print(f"Weighted F1@LowRisk (ä½å±åŠ æƒF1): {low_risk_weighted_f1:.4f}")
    except Exception as e:
        low_risk_macro_f1 = 0.0
        low_risk_weighted_f1 = 0.0
        print(f"ä½å±Macro/Weighted F1è®¡ç®—å¤±è´¥: {str(e)}")
    
    # 5. Cost-aware Error (æˆæœ¬åŠ æƒè¯¯å·®)
    # å°†ä½é£é™©è¯¯åˆ¤ä¸ºé«˜é£é™©çš„æˆæœ¬è®¾ä¸ºæœ€é«˜
    cost_matrix = np.array([
        [0, 1, 2, 4, 8],    # çœŸå®=1, é¢„æµ‹ä¸º1,2,3,4,5çš„æˆæœ¬
        [1, 0, 1, 3, 6],    # çœŸå®=2
        [2, 1, 0, 2, 4],    # çœŸå®=3  
        [1, 2, 1, 0, 1],    # çœŸå®=4
        [2, 3, 2, 1, 0]     # çœŸå®=5
    ])
    
    total_cost = 0
    for i, true_val in enumerate(y_true):
        pred_val = y_pred[i]
        cost = cost_matrix[true_val-1, pred_val-1]
        total_cost += cost
    
    avg_cost = total_cost / len(y_true)
    print(f"Cost-aware Error (å¹³å‡æˆæœ¬): {avg_cost:.4f}")
    
    return {
        'high_risk_recall': high_risk_recall,
        'high_risk_precision': high_risk_precision,
        'high_risk_macro_f1': high_risk_macro_f1,
        'high_risk_weighted_f1': high_risk_weighted_f1,
        'low_risk_recall': low_risk_recall,
        'low_risk_precision': low_risk_precision,
        'low_risk_macro_f1': low_risk_macro_f1,
        'low_risk_weighted_f1': low_risk_weighted_f1,
        'fpr_low_risk': fpr_low_risk,
        'avg_cost': avg_cost
    }

def plot_confusion_matrix(y_true, y_pred, save_path=None):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶åˆ†æé”™ä½æ–¹å‘"""
    print("\\n=== æ··æ·†çŸ©é˜µåˆ†æ ===")
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred, labels=[1,2,3,4,5])
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç»å¯¹æ•°é‡æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5], ax=ax1)
    ax1.set_title('æ··æ·†çŸ©é˜µ (ç»å¯¹æ•°é‡)')
    ax1.set_xlabel('é¢„æµ‹è¯„åˆ†')
    ax1.set_ylabel('çœŸå®è¯„åˆ†')
    
    # æ¯”ä¾‹æ··æ·†çŸ©é˜µ
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5], ax=ax2)
    ax2.set_title('æ··æ·†çŸ©é˜µ (è¡Œå½’ä¸€åŒ–æ¯”ä¾‹)')
    ax2.set_xlabel('é¢„æµ‹è¯„åˆ†')
    ax2.set_ylabel('çœŸå®è¯„åˆ†')
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    # åˆ†æé”™ä½æ–¹å‘
    print("\\næ··æ·†çŸ©é˜µ (ç»å¯¹æ•°é‡):")
    print("çœŸå®\\\\é¢„æµ‹", end="")
    for j in range(5):
        print(f"{j+1:6d}", end="")
    print()
    
    for i in range(5):
        print(f"{i+1:8d}", end="")
        for j in range(5):
            print(f"{cm[i][j]:6d}", end="")
        print()
    
    # é”™ä½æ–¹å‘åˆ†æ
    print("\\n=== é”™ä½æ–¹å‘åˆ†æ ===")
    
    # é«˜ä¼°ï¼ˆé¢„æµ‹é«˜äºçœŸå®ï¼‰
    overestimate = 0
    underestimate = 0
    correct = 0
    
    for i in range(5):
        for j in range(5):
            if i < j:  # é¢„æµ‹é«˜äºçœŸå®
                overestimate += cm[i][j]
            elif i > j:  # é¢„æµ‹ä½äºçœŸå®  
                underestimate += cm[i][j]
            else:  # é¢„æµ‹æ­£ç¡®
                correct += cm[i][j]
    
    total = overestimate + underestimate + correct
    print(f"é¢„æµ‹æ­£ç¡®: {correct} ({correct/total*100:.1f}%)")
    print(f"é«˜ä¼° (é¢„æµ‹>çœŸå®): {overestimate} ({overestimate/total*100:.1f}%)")
    print(f"ä½ä¼° (é¢„æµ‹<çœŸå®): {underestimate} ({underestimate/total*100:.1f}%)")
    
    # ä¸¥é‡é”™ä½åˆ†æï¼ˆå·®è·>=2ï¼‰
    severe_errors = 0
    for i in range(5):
        for j in range(5):
            if abs(i - j) >= 2:
                severe_errors += cm[i][j]
    
    print(f"ä¸¥é‡é”™ä½ (|é¢„æµ‹-çœŸå®|>=2): {severe_errors} ({severe_errors/total*100:.1f}%)")
    
    return cm

def analyze_by_risk_level(df):
    """æŒ‰é£é™©ç­‰çº§è¯¦ç»†åˆ†æ"""
    print("\\n=== æŒ‰é£é™©ç­‰çº§è¯¦ç»†åˆ†æ ===")
    
    risk_levels = {
        1: "ä½é£é™©", 2: "è¾ƒä½é£é™©", 3: "ä¸­ç­‰é£é™©", 
        4: "è¾ƒé«˜é£é™©", 5: "é«˜é£é™©"
    }
    
    for level in [1, 2, 3, 4, 5]:
        subset = df[df['human_rating_int'] == level]
        if len(subset) == 0:
            continue
            
        print(f"\\n{risk_levels[level]} (çœŸå®è¯„åˆ†={level}) - {len(subset)}ä¸ªæ ·æœ¬:")
        
        # é¢„æµ‹åˆ†å¸ƒ
        pred_dist = subset['rating_int'].value_counts().sort_index()
        for pred_level, count in pred_dist.items():
            percentage = count / len(subset) * 100
            print(f"  é¢„æµ‹ä¸º{pred_level}: {count}ä¸ª ({percentage:.1f}%)")
        
        # å‡†ç¡®ç‡
        accuracy = (subset['human_rating_int'] == subset['rating_int']).mean()
        print(f"  å‡†ç¡®ç‡: {accuracy:.3f}")
        
        # å¹³å‡é¢„æµ‹è¯¯å·®
        mae = np.mean(np.abs(subset['human_rating_int'] - subset['rating_int']))
        print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {mae:.3f}")

def plot_distribution_comparison(df, save_path=None):
    """ç»˜åˆ¶åˆ†å¸ƒå¯¹æ¯”å›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. æ•´ä½“åˆ†å¸ƒå¯¹æ¯”
    ax1 = axes[0, 0]
    x = np.arange(1, 6)
    true_dist = df['human_rating_int'].value_counts().sort_index()
    pred_dist = df['rating_int'].value_counts().sort_index()
    
    width = 0.35
    ax1.bar(x - width/2, [true_dist.get(i, 0) for i in range(1, 6)], 
            width, label='äººå·¥æ ‡æ³¨', alpha=0.8)
    ax1.bar(x + width/2, [pred_dist.get(i, 0) for i in range(1, 6)], 
            width, label='é¢„æµ‹ç»“æœ', alpha=0.8)
    ax1.set_xlabel('è¯„åˆ†')
    ax1.set_ylabel('æ•°é‡')
    ax1.set_title('è¯„åˆ†åˆ†å¸ƒå¯¹æ¯”')
    ax1.legend()
    ax1.set_xticks(x)
    
    # 2. æ•£ç‚¹å›¾
    ax2 = axes[0, 1]
    ax2.scatter(df['human_rating_int'], df['rating_int'], alpha=0.6)
    ax2.plot([1, 5], [1, 5], 'r--', label='å®Œç¾é¢„æµ‹çº¿')
    ax2.set_xlabel('äººå·¥æ ‡æ³¨')
    ax2.set_ylabel('é¢„æµ‹ç»“æœ')
    ax2.set_title('é¢„æµ‹ vs çœŸå® æ•£ç‚¹å›¾')
    ax2.legend()
    ax2.set_xlim(0.5, 5.5)
    ax2.set_ylim(0.5, 5.5)
    
    # 3. è¯¯å·®åˆ†å¸ƒ
    ax3 = axes[1, 0]
    errors = df['rating_int'] - df['human_rating_int']
    ax3.hist(errors, bins=np.arange(-4.5, 5.5, 1), alpha=0.7, edgecolor='black')
    ax3.set_xlabel('é¢„æµ‹è¯¯å·® (é¢„æµ‹-çœŸå®)')
    ax3.set_ylabel('é¢‘æ¬¡')
    ax3.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
    ax3.axvline(x=0, color='red', linestyle='--', label='é›¶è¯¯å·®')
    ax3.legend()
    
    # 4. æŒ‰çœŸå®è¯„åˆ†çš„é¢„æµ‹å‡†ç¡®æ€§
    ax4 = axes[1, 1]
    accuracies = []
    for level in range(1, 6):
        subset = df[df['human_rating_int'] == level]
        if len(subset) > 0:
            acc = (subset['human_rating_int'] == subset['rating_int']).mean()
            accuracies.append(acc)
        else:
            accuracies.append(0)
    
    ax4.bar(range(1, 6), accuracies, alpha=0.8)
    ax4.set_xlabel('çœŸå®è¯„åˆ†')
    ax4.set_ylabel('å‡†ç¡®ç‡')
    ax4.set_title('å„è¯„åˆ†ç­‰çº§é¢„æµ‹å‡†ç¡®ç‡')
    ax4.set_xticks(range(1, 6))
    ax4.set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, acc in enumerate(accuracies):
        ax4.text(i+1, acc + 0.01, f'{acc:.3f}', ha='center')
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def generate_report(main_metrics, aux_metrics, total_samples):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    print("\\n" + "="*60)
    print("åæ¬ºè¯ˆæ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    print(f"\\næ•°æ®æ¦‚å†µ:")
    print(f"- æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    print(f"\\nä¸»è¦æŒ‡æ ‡:")
    print(f"- QWK (Quadratic Weighted Kappa): {main_metrics['qwk']:.4f} ({main_metrics['qwk_interpretation']})")
    print(f"- MAE (Mean Absolute Error): {main_metrics['mae']:.4f}")
    print(f"- Weighted F1: {main_metrics['weighted_f1']:.4f}")
    print(f"- Macro F1: {main_metrics['macro_f1']:.4f}")
    
    print(f"\\nè¾…åŠ©æŒ‡æ ‡:")
    print(f"- é«˜é£é™©å¬å›ç‡ (Recall@HighRisk): {aux_metrics['high_risk_recall']:.4f}")
    print(f"- é«˜é£é™©ç²¾ç¡®ç‡ (Precision@HighRisk): {aux_metrics['high_risk_precision']:.4f}")
    print(f"- é«˜é£é™©å®å¹³å‡F1 (Macro F1@HighRisk): {aux_metrics['high_risk_macro_f1']:.4f}")
    print(f"- é«˜é£é™©åŠ æƒF1 (Weighted F1@HighRisk): {aux_metrics['high_risk_weighted_f1']:.4f}")
    print(f"- ä½é£é™©å¬å›ç‡ (Recall@LowRisk): {aux_metrics['low_risk_recall']:.4f}")
    print(f"- ä½é£é™©ç²¾ç¡®ç‡ (Precision@LowRisk): {aux_metrics['low_risk_precision']:.4f}")
    print(f"- ä½é£é™©å®å¹³å‡F1 (Macro F1@LowRisk): {aux_metrics['low_risk_macro_f1']:.4f}")
    print(f"- ä½é£é™©åŠ æƒF1 (Weighted F1@LowRisk): {aux_metrics['low_risk_weighted_f1']:.4f}")
    print(f"- ä½é£é™©è¯¯åˆ¤ç‡ (FPR@LowRisk): {aux_metrics['fpr_low_risk']:.4f}")
    print(f"- æˆæœ¬åŠ æƒè¯¯å·® (Cost-aware Error): {aux_metrics['avg_cost']:.4f}")
    
    print(f"\\næ¨¡å‹è¯„ä¼°ç»“è®º:")
    
    # QWKè¯„ä¼°
    if main_metrics['qwk'] >= 0.8:
        print("âœ… QWKè¡¨ç°ä¼˜ç§€ï¼Œæ¨¡å‹é¢„æµ‹ä¸äººå·¥æ ‡æ³¨é«˜åº¦ä¸€è‡´")
    elif main_metrics['qwk'] >= 0.6:
        print("ğŸŸ¡ QWKè¡¨ç°è‰¯å¥½ï¼Œæ¨¡å‹é¢„æµ‹åŸºæœ¬å¯é ")
    else:
        print("âŒ QWKè¡¨ç°ä¸ä½³ï¼Œæ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # MAEè¯„ä¼°
    if main_metrics['mae'] <= 0.5:
        print("âœ… MAEè¡¨ç°ä¼˜ç§€ï¼Œå¹³å‡è¯¯å·®å¾ˆå°")
    elif main_metrics['mae'] <= 1.0:
        print("ğŸŸ¡ MAEè¡¨ç°ä¸­ç­‰ï¼Œå­˜åœ¨ä¸€å®šè¯¯å·®")
    else:
        print("âŒ MAEè¾ƒå¤§ï¼Œé¢„æµ‹è¯¯å·®æ˜æ˜¾")
    
    # é«˜é£é™©æ£€æµ‹è¯„ä¼°
    if aux_metrics['high_risk_recall'] >= 0.9:
        print("âœ… é«˜é£é™©æ£€æµ‹èƒ½åŠ›ä¼˜ç§€ï¼Œæ¼æ£€ç‡ä½")
    elif aux_metrics['high_risk_recall'] >= 0.8:
        print("ğŸŸ¡ é«˜é£é™©æ£€æµ‹èƒ½åŠ›è‰¯å¥½")
    else:
        print("âŒ é«˜é£é™©æ£€æµ‹èƒ½åŠ›ä¸è¶³ï¼Œå¯èƒ½å­˜åœ¨æ¼æ£€é—®é¢˜")
    
    # è¯¯åˆ¤ç‡è¯„ä¼°
    if aux_metrics['fpr_low_risk'] <= 0.05:
        print("âœ… ä½é£é™©è¯¯åˆ¤ç‡å¾ˆä½ï¼Œè¯¯æŠ¥æ§åˆ¶è‰¯å¥½")
    elif aux_metrics['fpr_low_risk'] <= 0.1:
        print("ğŸŸ¡ ä½é£é™©è¯¯åˆ¤ç‡è¾ƒä½")
    else:
        print("âŒ ä½é£é™©è¯¯åˆ¤ç‡åé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦æ•æ„Ÿé—®é¢˜")
    
    # Macro F1æŒ‡æ ‡è¯„ä¼°
    if aux_metrics['high_risk_macro_f1'] >= 0.8:
        print("âœ… é«˜é£é™©å®å¹³å‡F1åˆ†æ•°ä¼˜ç§€ï¼Œé«˜å±æ£€æµ‹ç»¼åˆæ€§èƒ½è‰¯å¥½")
    elif aux_metrics['high_risk_macro_f1'] >= 0.6:
        print("ğŸŸ¡ é«˜é£é™©å®å¹³å‡F1åˆ†æ•°è‰¯å¥½")
    else:
        print("âŒ é«˜é£é™©å®å¹³å‡F1åˆ†æ•°åä½ï¼Œéœ€è¦åœ¨å¬å›ç‡å’Œç²¾ç¡®ç‡é—´å¹³è¡¡")
    
    if aux_metrics['low_risk_macro_f1'] >= 0.8:
        print("âœ… ä½é£é™©å®å¹³å‡F1åˆ†æ•°ä¼˜ç§€ï¼Œä½å±è¯†åˆ«ç»¼åˆæ€§èƒ½è‰¯å¥½")
    elif aux_metrics['low_risk_macro_f1'] >= 0.6:
        print("ğŸŸ¡ ä½é£é™©å®å¹³å‡F1åˆ†æ•°è‰¯å¥½")
    else:
        print("âŒ ä½é£é™©å®å¹³å‡F1åˆ†æ•°åä½ï¼Œéœ€è¦ä¼˜åŒ–ä½å±æ ·æœ¬è¯†åˆ«èƒ½åŠ›")

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    input_file = "result.csv"
    
    print("åæ¬ºè¯ˆæ¨¡å‹è¯„ä¼°è„šæœ¬")
    print("="*50)
    
    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    df = load_and_preprocess_data(input_file)
    
    if len(df) == 0:
        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œè¯„ä¼°")
        return
    
    # æå–è¯„åˆ†æ•°æ®
    y_true = df['human_rating_int'].values
    y_pred = df['rating_int'].values
    
    # 2. è®¡ç®—ä¸»è¦æŒ‡æ ‡
    main_metrics = calculate_main_metrics(y_true, y_pred)
    
    # 3. è®¡ç®—è¾…åŠ©æŒ‡æ ‡  
    aux_metrics = calculate_auxiliary_metrics(y_true, y_pred)
    
    # 4. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = plot_confusion_matrix(y_true, y_pred, "confusion_matrix.png")
    
    # 5. æŒ‰é£é™©ç­‰çº§åˆ†æ
    analyze_by_risk_level(df)
    
    # 6. ç»˜åˆ¶åˆ†å¸ƒå¯¹æ¯”å›¾
    plot_distribution_comparison(df, "distribution_comparison.png")
    
    # 7. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    generate_report(main_metrics, aux_metrics, len(df))
    
    # 8. ä¿å­˜è¯¦ç»†ç»“æœ
    results = {
        **main_metrics,
        **aux_metrics,
        'total_samples': len(df)
    }
    
    # ä¿å­˜ä¸ºJSON
    import json
    with open('evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nè¯„ä¼°å®Œæˆï¼")
    print(f"- è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: evaluation_results.json")
    print(f"- æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°: confusion_matrix.png") 
    print(f"- åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: distribution_comparison.png")

if __name__ == "__main__":
    main()